Context Engineering is new term gaining traction in the AI world. The conversation is shifting from "prompt engineering" to a broader, more powerful concept: **Context Engineering**. [Tobi Lutke](https://x.com/tobi/status/1935533422589399127) describes it as "the art of providing all the context for the task to be plausibly solvable by the LLM.” and he is right.

With the rise of Agents it becomes more important what information we load into the “limited working memory”. We are seeing that the main thing that determines whether an Agents succeeds or fails is the quality of the context you give it. Most agent failures are not model failures anyemore, they are context failures.

## [](https://www.philschmid.de/context-engineering#what-is-the-context)What is the Context?

To understand context engineering, we must first expand our definition of "context." It isn't just the single prompt you send to an LLM. Think of it as everything the model sees before it generates a response.

![Context](https://www.philschmid.de/static/blog/context-engineering/context.png)

- **Instructions / System Prompt:** An initial set of instructions that define the behavior of the model during a conversation, can/should include examples, rules ….
- **User Prompt:** Immediate task or question from the user.
- **State / History (short-term Memory):** The current conversation, including user and model responses that have led to this moment.
- **Long-Term Memory:** Persistent knowledge base, gathered across many prior conversations, containing learned user preferences, summaries of past projects, or facts it has been told to remember for future use.
- **Retrieved Information (RAG):** External, up-to-date knowledge, relevant information from documents, databases, or APIs to answer specific questions.
- **Available Tools:** Definitions of all the functions or built-in tools it can call (e.g., check_inventory, send_email).
- **Structured Output:** Definitions on the format of the model's response, e.g. a JSON object.

## [](https://www.philschmid.de/context-engineering#why-it-matters-from-cheap-demo-to-magical-product)Why It Matters: From Cheap Demo to Magical Product

The secret to building truly effective AI agents has less to do with the complexity of the code you write, and everything to do with the quality of the context you provide.

Building Agents is less about the code you write or framework you use. The difference between a cheap demo and a “magical” agent is about the quality of the context you provide. Imagine an AI assistant is asked to schedule a meeting based on a simple email:

> Hey, just checking if you’re around for a quick sync tomorrow.

**The "Cheap Demo" Agent** has poor context. It sees only the user's request and nothing else. Its code might be perfectly functional—it calls an LLM and gets a response—but the output is unhelpful and robotic:

> Thank you for your message. Tomorrow works for me. May I ask what time you had in mind?

**The "Magical" Agent** is powered by rich context. The code's primary job isn't to figure out _how_ to respond, but to _gather the information_ the LLM needs to full fill its goal. Before calling the LLM, you would extend the context to include

- Your calendar information (which shows you're fully booked).
- Your past emails with this person (to determine the appropriate informal tone).
- Your contact list (to identify them as a key partner).
- Tools for send_invite or send_email.

Then you can generate a response.

> Hey Jim! Tomorrow’s packed on my end, back-to-back all day. Thursday AM free if that works for you? Sent an invite, lmk if it works.

The magic isn't in a smarter model or a more clever algorithm. It’s in about providing the right context for the right task. This is why context engineering will matter. Agent failures aren't only model failures; they are context failures.

## [](https://www.philschmid.de/context-engineering#from-prompt-to-context-engineering)From Prompt to Context Engineering

What is context engineering? While "prompt engineering" focuses on crafting the perfect set of instructions in a single text string, context engineering is a far broader. Let's put it simply:

> Context Engineering is the discipline of designing and building dynamic systems that provides the right information and tools, in the right format, at the right time, to give a LLM everything it needs to accomplish a task.

Context Engineering is

- **A System, Not a String:** Context isn't just a static prompt template. It’s the output of a **system** that runs _before_ the main LLM call.
- **Dynamic:** Created on the fly, tailored to the immediate task. For one request this could be the calendar data for another the emails or a web search.
- **About the right information, tools at the right time:** The core job is to ensure the model isn’t missing crucial details ("Garbage In, Garbage Out"). This means providing both knowledge (information) and capabilities (tools) only when required and helpful.
- **where the format matters:** How you present information matters. A concise summary is better than a raw data dump. A clear tool schema is better than a vague instruction.

## [](https://www.philschmid.de/context-engineering#conclusion)Conclusion

Building powerful and reliable AI Agents is becoming less about finding a magic prompt or model updates. It is about the engineering of context and providing the right information and tools, in the right format, at the right time. It’s a cross-functional challenge that involves understanding your business use case, defining your outputs, and structuring all the necessary information so that an LLM can “accomplish the task."

 为什么上下文需要“工程”？

  

现在的上下文窗口已经普遍达到128K以上甚至1M，这给了智能体勃勃生机的无线可能：你可以把大量智能体需要的东西（指令、知识、工具等）都丢进一次Prompt中，让LLM自由的推理、规划、行动。

但为什么智能体的表现仍然不尽如人意？问题的原因不在于上下文能“装”多少，而在于最终LLM“消化”的是什么。LLM的输出永远存在不确定性——这是架构层面注定的事实。在模型尚未发生质变之前，我们能控制的唯一变量，就是它的输入：上下文（Context）**。**

![图片](https://mmbiz.qpic.cn/mmbiz_png/90CnTjsKiae6iaAd07O0LB8gxRaNWbQQYs0oUePlIusY9ic4icv8bUfZvJ19BWLVkl4VFDdTAw7IoS1k2QMEGRUYug/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=1)

而控制上下文，更大的“胃口”（窗口大小）固然重要；但更重要的是“营养”（质量）。大量的测试表明，更长的上下文并不确定会带来更好的结果：其中可能的冗余、冲突、错误（幻觉）信息都会让LLM迷失方向。

**即使上下文内容完全正确**，当输入规模过大时，LLM的“注意力”也可能会被分散，关键指令或语义线索被淹没，导致模型理解出现偏移与错位，甚至在推理中“跑题”。

更重要的是，在智能体的多轮推理中，微小的偏差会不断“叠加”，最终Agent任务可能完全“南辕北辙”。

这就是为什么需要上下文工程（Context Engineering）：

当LLM可以“吞”下更多内容时，你需要确保“营养搭配”：有结构、有逻辑、有重点，而不仅仅是“堆料”。

  

 什么是上下文工程？

  

Shopify的CEO Tobi Lutke这样形容上下文工程：

为LLM提供所有用于合理解决任务的上下文信息的艺术。

![图片](https://mmbiz.qpic.cn/mmbiz_png/90CnTjsKiae6iaAd07O0LB8gxRaNWbQQYsuxLhCbhicNPw6QfoV9tCuIEEAYNoZ1eIedsicRhdQnUxr5haQ4lK2QiaA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=18)

如果说上下文是LLM的RAM（来自OpenAI 前研究科学家 Karpathy的比喻），那么**上下文工程（Context Engineering）** 就是 — 设计并实施这块有限 RAM 的管理机制，让信息的组织、输入、更新、淘汰都更有秩序与目的性。

上下文工程不是简单的提示模板设计与堆叠（尽管提示模板是它的一角）。它需要关注到更深层次的一系列结构性问题与工程方法：

- 包含哪些信息，如何组织，什么格式？
    
    行业知识或工具指南，或者更多，如何分类排序？
    
    普通文本还是半结构化的JSON、Markdown？
    
      
    
- 信息从哪里，又在什么时候获取？
    
    是来自企业内部数据库还是互联网、或Agent产生？
    
    是原始输入还是通过中间步骤动态注入？
    
      
    
- 信息如何产生，如何存储，又是如何检索？
    
    静态信息、缓存信息、动态生成与检索？
    
    传统索引、向量数据库，还是实时 Web 搜索？
    
      
    

- 信息是否需要清洗、标注、压缩？
    
    是否需要去除冗余、噪音、重复，并标注出重点？
    
    上下文是否需要压缩（如长期记忆）？
    
      
    
- 信息是否需要有生命周期管理？
    
    哪些内容持久存在，哪些随时间与任务衰减？
    
    信息过期机制，滑动窗口、基于使用次数还是LLM判断？
    

总之，上下文工程更像是为LLM建立一套语义的“供应链”，让LLM得以正确理解和推理 — 从而让Agent做出正确的决策。

  

 上下文工程的常见策略

  

上下文工程涵盖很多不同的方法与技术。在实践中，有一些经过验证、可复用的策略，能帮助我们更好地实施上下文工程，这里为大家总结：  

![图片](https://mmbiz.qpic.cn/mmbiz_png/90CnTjsKiae43Q38OcxIKHaK35J8IOMNeGyLg5iaykF0F3zNtiaHAmZdJSzbIq63O2Nk2icmeAibFrLnBdwvJylWXibQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1#imgIndex=20)

1）上下文分区与隔离

为了让“RAM”中的信息不打架、逻辑不混乱，可以对上下文进行分区：

- 将上下文划分为**指令层、目标层、记忆层、知识层、工具层**等
    
- 为每一层**分配独立的 Token 预算与出现顺序**，确保关键信息始终可见
    
- 给不同的角色（如规划器、执行器、反思器）分配不同的上下文切片，避免信息冗余与推理“漂移”
    
      
    

2）用RAG装备知识

- 通过检索召回相关的知识，动态注入上下文。
    
- 不要迷信超大上下文窗口 -- “把所有东西都塞进窗口”。太多的垃圾信息，只会让LLM又傻又慢。上下文不是简单的“堆料”，而是“选料”。
    
      
    

3）合理组装工具（Tools）

工具（Tools）是Agent的手脚，上下文工程要教他如何用手脚：

- 给每个工具以清晰、没有歧义的“说明书”（如函数Docstring）
    
- 仅装载最相关的工具，比如用RAG来筛选工具描述
    
- 让工具的异常返回具有清晰的“引导性”，而不是简单的“Tool Error”
    
      
    

4）多智能体协作

在大型的Agent任务中，多Agent是一种常见的架构模式：

- 每个子Agent拥有各自独立的上下文窗口与任务焦点，本身就是一种上下文隔离，减少干扰的手段
    
- 让单Agent推理100个工具，不如让五个Agent各推理20个，再加一个协调者
    
- 多Agent设计可以遵循模块设计经典原则“高内聚、低耦合”
    
      
    

5）修剪、卸载、压缩

在长程推理任务的Agent中，你需要考虑应对上下文的“滚雪球”式的膨胀：

- 修剪：适时删除不相关的信息。可借助LLM或者规则引擎来判断
    
- 卸载：让Agent在上下文之外”记笔记”，比如把阶段性结果写入外部存储，必要时再引用（如工具输出、动态规则等）
    
- 压缩：通过摘要或语义聚合让上下文更紧凑，提高信息密度而不过度冗长。
    
      
    

当然，除了这些重要的上下文策略，传统提示工程中的技巧仍然奏效：比如指令既不能太死板，也不能太空洞；提供少量但多样化的few-shot等。

  
